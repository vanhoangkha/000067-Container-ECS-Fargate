[
{
	"uri": "/3-containersanddocker/3.1-dockerbasic/",
	"title": "Basic Docker",
	"tags": [],
	"description": "",
	"content": "Basic Docker Overview Docker is a software platform that allows you to quickly build, test, and deploy applications. Docker packages software into standardized units called containers that have everything the software needs to run, including libraries, system tools, code, and runtimes. Using Docker, you can quickly deploy and scale your application into any environment and know with certainty that your code will run. Running Docker on AWS gives developers and administrators a low-cost and highly reliable way to build, ship, and run distributed applications of any size.\nDocker partners with AWS to help developers quickly bring modern applications to the cloud. This partnership helps developers using Docker Compose and Docker Desktop to leverage the same local workflows they use today to seamlessly deploy applications on Amazon ECS and AWS Fargate.\n\rHow Docker works Docker works by providing a standard method for running your code. Docker is an operating system for containers. In the same way that a virtual machine virtualizes (eliminating the need to directly manage) server hardware, containers virtualize the host\u0026rsquo;s operating system. Docker is installed on each host and provides simple commands that you can use to build, start, or stop containers.\nAWS services like AWS Fargate, Amazon ECS, Amazon EKS, and AWS Batch make it easy to run Docker containers at scale.\nIn the lab, we execute basic Docker commands with AWS Cloud9\n Check the client and server are working with the command:  docker --version Docker container is built from image.   command pull  docker pull [OPTIONS] NAME[: TAG|@DIGEST]  First of all, we use the docker pull nginx:latest command to pull the latest nginx image from Docker Hub.  docker pull nginx\\:latest To test pull the image successfully (image will be in the local machine Docker cache).   Usage  docker images [OPTIONS] [REPOSITORY[: TAG]]  Purpose of listing images.  docker images Docker Daemon acts as server, receives RESTful requests from Docker Client and executes.   We use the command docker run -d -p 8080:80 \u0026ndash;name nginx nginx:latest.  docker run -d -p 8080:80 --name nginx nginx\\:latest   -d: Run the container in the background\n  Name the container as nginx\n  -p 8080:80: Expose port 80 of container to port 8080 of machine host\n  nginx:latest: Container launched from image is nginx:latest\n  Reference command to run a container:\n  docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Check the nginx containers running with the command:  docker ps  To perform the list of containers we execute the command:  docker ps [OPTIONS] Use command curl http://localhost:8080 to use nginx container and verify it is working with index.html  curl http://localhost:8080  Refer to the basic curl command:  curl [options/URLs] \rAlso you can read more about CURL\n\rTo view nginx and container logs.   We use the docker logs nginx command. curl request event occurs  docker logs nginx  Refer to the fetch log command of the container:   docker logs [OPTIONS] CONTAINER Next, execute the command docker exec -it nginx /bin/bash to interact with container filesystem and constraints  docker exec -it nginx /bin/bash  Consult the interactive command in the running container:  docker exec [OPTIONS] CONTAINER COMMAND [ARG...] We perform the content view of nginx with the command:  cd /usr/share/nginx/html\rcat index.html Use exit to get out  To stop running the container we execute the command  docker stop nginx  Refer to the command to stop one or more containers:  docker stop [OPTIONS] CONTAINER [CONTAINER...] Use docker ps -a to view container (stopped container) to restart use command: docker start nginx  docker ps -a Perform container deletion (input is container ID or container name)  docker rm nginx  Refer to the command to delete one or more containers:  docker rm [OPTIONS] CONTAINER [CONTAINER...] Perform nginx image deletion with command (deleted image format can be name: tag or IMAGE ID)  docker rmi nginx\\:latest  Refer to the command to delete one or more images:   docker rmi [OPTIONS] IMAGE [IMAGE...] "
},
{
	"uri": "/2-prerequiste/2.1-createcloudformation/",
	"title": "Create CF Stack ",
	"tags": [],
	"description": "",
	"content": "Create CloudFormation Stack  Go to Deploy to AWS to proceed with stack creation.  Alternatively, you can also create a CloudFormation Stack with Upload yaml file\n\r The first step, configure the template. Select Template is ready Template source, select Amazon S3 URL In the lab, preconfigured Amazon S3 URL Select Next  We configure the stack details   Stack name, enter the stack name you want to set.  STACK_NAME will be used as the header or header of the names of the services in this lab. Example: alb-STACK_NAME-XXX\n\r In the Parameters section, select false for SkipBucket Select Next  Proceed to Configure stack options   Tags, enter the value key-value (you can enter it)  Select Next  Check configuration stack   Select I acknowledge that AWS CloudFormation might create IAM resources Select Create Stack  Stack creation takes about 7 minutes.   Select stack to create successfully Select Events See the stack creation process  This stack initializes Cloud9 Environment, DynamoDB Table, LoadBalancerDNS, ProfileName, S3WebsiteURL, SiteBucket.\n\rIn the newly created stack interface   Select Outputs Select value as the path Cloud9 of Key Cloud9Env.  We use AWS Cloud9 as our development environment.  We will be using this Cloud9 Environment throughout the whole lab. The AWS Cloud9 environment created by CloudFormation is named Project- STACK_NAME\n\rCheck Outputs of the stack   Access to S3 Select Buckets Bucket has been created (http://BUCKET_NAME.s3-website.REGION.amazonaws.com/) The website is static and the link is saved in workshop-1/cfn-outputs.json  Next we check DynamoDB   Access to DynamoDB New Table has been initialized  Check Load Balancers   Access to EC2 Select Load Balancers Check the result  "
},
{
	"uri": "/7-microservices/7.1-createnewrevision/",
	"title": "Create Revision",
	"tags": [],
	"description": "",
	"content": "Create Revision  In the First step, we add some glue code in monolith to turn the like function into a separate service (microservice). In the lab, use Cloud9 and find the app/monolith-service/service/mythicalMysfitsService.py file. Then do uncomment the following code:  # @app.route(\u0026#34;/mysfits/\u0026lt;mysfit_id\u0026gt;/fulfill-like\u0026#34;, methods=[\u0026#39;POST\u0026#39;])\r# def fulfillLikeMysfit(mysfit_id):\r# serviceResponse = mysfitsTableClient.likeMysfit(mysfit_id)\r# flaskResponse = Response(serviceResponse)\r# flaskResponse.headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json\u0026#34;\r# return flaskResponse With the new feature added monolith, we rebuild the monolith docker image with a new tag (nolike).   cd ~/environment/amazon-ecs-mythicalmysfits-workshop/workshop-1/app/monolith-service\rMONO_ECR_REPOSITORY_URI=$(aws ecr describe-repositories | jq -r .repositories[].repositoryUri | grep mono)\rdocker build -t monolith-service:nolike . Execute push monolith docker image onto ECR   It is best to avoid the latest tag, which can be ambiguous. Instead, choose a unique tag, descriptive name, or preferably a Git SHA and/or version ID).   docker tag monolith-service:nolike $MONO_ECR_REPOSITORY_URI:nolike\rdocker push $MONO_ECR_REPOSITORY_URI:nolike Check whether the monolith docker image has been pushed to the ECR?   Access to ECS Select Task Definitions Select Monolith-Definitions-STACK_NAME (save you may be different in the picture because the following part depends on the STACK_NAME you set)  Then select Monolith-Definition-STACK_NAME revision 2.   Select Create new revision  Go to ECR to view the repositories. Now, in the Images section, there are 2 images with 2 tags: latest and nolike.   Copy Image URI  Use Image URI tag nolike to create New revision   Configure Container Replace Image URI with Image URI tag nolike  Select Create  Finish creating Monolith-Definition-STACK_NAME revision 3.  "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction Overview Docker is a software platform that allows you to quickly build, test, and deploy applications. Docker packages software into standardized units called containers that have everything the software needs to run, including libraries, system tools, code, and runtimes. Using Docker, you can quickly deploy and scale your application into any environment and know with certainty that your code will run. Running Docker on AWS gives developers and administrators a low-cost and highly reliable way to build, ship, and run distributed applications of any size.\nDocker partners with AWS to help developers quickly bring modern applications to the cloud. This partnership helps developers using Docker Compose and Docker Desktop to leverage the same local workflows they use today to seamlessly deploy applications on Amazon ECS and AWS Fargate.\n\rHow Docker works Docker works by providing a standard method for running your code. Docker is an operating system for containers. In the same way that a virtual machine virtualizes (eliminating the need to directly manage) server hardware, containers virtualize the host\u0026rsquo;s operating system. Docker is installed on each host and provides simple commands that you can use to build, start, or stop containers.\nAWS services like AWS Fargate, Amazon ECS, Amazon EKS, and AWS Batch make it easy to run Docker containers at scale.\nIn the lab, we will be implementing Monolith to Microservices with Docker and AWS Fargate\n"
},
{
	"uri": "/",
	"title": "Monolith to Microservices with Docker and AWS Fargate",
	"tags": [],
	"description": "",
	"content": "Monolith to Microservices with Docker and AWS Fargate Overview Docker is a software platform that allows you to quickly build, test, and deploy applications. Docker packages software into standardized units called containers that have everything the software needs to run, including libraries, system tools, code, and runtimes. Using Docker, you can quickly deploy and scale your application into any environment and know with certainty that your code will run. Running Docker on AWS gives developers and administrators a low-cost and highly reliable way to build, ship, and run distributed applications of any size.\nDocker partners with AWS to help developers quickly bring modern applications to the cloud. This partnership helps developers using Docker Compose and Docker Desktop to leverage the same local workflows they use today to seamlessly deploy applications on Amazon ECS and AWS Fargate.\n\rHow Docker works Docker works by providing a standard method for running your code. Docker is an operating system for containers. In the same way that a virtual machine virtualizes (eliminating the need to directly manage) server hardware, containers virtualize the host\u0026rsquo;s operating system. Docker is installed on each host and provides simple commands that you can use to build, start, or stop containers.\nAWS services like AWS Fargate, Amazon ECS, Amazon EKS, and AWS Batch make it easy to run Docker containers at scale.\nIn the lab, we will be implementing Monolith to Microservices with Docker and AWS Fargate\n"
},
{
	"uri": "/4-monolith/4.1-checkcloudformation/",
	"title": "Testing CloudFormation",
	"tags": [],
	"description": "",
	"content": "Check CloudFormation  Access to CloudFormation   Select the created stack Select Outputs Select S3WebsiteURL  Use your browser to access S3WebsiteURL.  About the website Mythical Mysfits Mythical Mysfits is a (fictional) pet adoption nonprofit dedicated to helping abandoned and often misunderstood mythical creatures find a forever new family! Mythical Mysfits believe that all creatures deserve a second chance, even if they take their first chance to hide under bridges and rob them of gratuitous acts of helplessness.\nOur business has thrived with a single Mysfits adoption center, located inside Devil Tower National Monument. Talk, friends, and join if you visit.\nWe\u0026rsquo;ve just had a bunch of new mystics arriving at our doorstep with nowhere else to go! They\u0026rsquo;re all pretty distraught after not only being evicted from their home\u0026hellip; but a grumpy goblin who also denied them all entry to a swamp they used to hide. hide in the past.\nThat\u0026rsquo;s why we hired you to be our Full Stack Engineer. We needed a more scalable way to showcase our treasure trove of mysteries and let families adopt them. We want you to build your first Mythical Mysfits adoption website to help introduce these adorable, wondrous, often mischievous creatures to the world!\n"
},
{
	"uri": "/2-prerequiste/2.2-setup/",
	"title": "Clone Repository",
	"tags": [],
	"description": "",
	"content": "Clone Repository  Clone Mythical Mysfits Workshop Repository   In the Cloud9 IDE interface, we use the following command to clone the repository:  git clone https://github.com/aws-samples/amazon-ecs-mythicalmysfits-workshop.git After cloning the repository, change the directory\u0026rsquo;s path:  cd amazon-ecs-mythicalmysfits-workshop/workshop-1 We execute the following command line to install the environment to prepare for the lab.  script/setup  This script will delete unnecessary Docker images to free up space. Also populate the DynamoDB table with original data. Upload web content to S3. Install some authentication mechanisms related to Docker.  #! /bin/bash\rset -eu\recho \u0026#34;Removing unneeded docker images...\u0026#34;\rdocker images -q | xargs docker rmi || true\recho \u0026#34;Installing dependencies...\u0026#34;\rsudo yum install -y jq\recho \u0026#34;Fetching CloudFormation outputs...\u0026#34;\rscript/fetch-outputs\recho \u0026#34;Populating DynamoDB table...\u0026#34;\rscript/load-ddb\recho \u0026#34;Uploading static site to S3...\u0026#34;\rif [[ $# -eq 1 ]]; then\rscript/upload-site $1\relse\rscript/upload-site\rfi\recho \u0026#34;Installing ECR Cred Helper...\u0026#34;\rsudo script/credhelper\recho \u0026#34;Attaching Instance Profile to Cloud9...\u0026#34;\rscript/associate-profile\recho \u0026#34;Success!\u0026#34; When you see \u0026ldquo;Success!\u0026rdquo; on the interface, the command has been executed successfully.  Check-in S3 interface   In the bucket have been uploaded website files  Test in the DynamoDB interface   The original data has been filled in Table DynamoDB  We should configure aws cli with our current region as default:  export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)\rexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;)\recho \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile\recho \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile\raws configure set default.region ${AWS_REGION}\raws configure get default.region "
},
{
	"uri": "/3-containersanddocker/3.2-containerimage/",
	"title": "Container image",
	"tags": [],
	"description": "",
	"content": "Execute container image In this part, we build the container image. Working with Dockerfiles.\nDockerfile is a text file with no extension, containing specifications for a software executable field, and structure for Docker Image. From those commands, Docker will build a Docker image (usually from a few MB to a few GB in size).\nDockerfile overview The general syntax of a Dockerfile\nINSTRUCTION arguments  INSTRUCTION is the name of the directives contained in Dockerfile, each directive performing a certain task, specified by Docker. When declaring these directives, they must be written in CAPITAL. A Dockerfile must begin with the FROM directive to declare which image will be used as the background to build your image. arguments is the body of the directives, which decides what the directive will do.  Some directives in Dockerfile FROM\nFROM \u0026lt;image\u0026gt; [AS \u0026lt;name\u0026gt;]\rFROM \u0026lt;image\u0026gt;[:\u0026lt;tag\u0026gt;] [AS \u0026lt;name\u0026gt;]\rFROM \u0026lt;image\u0026gt;[@\u0026lt;digest\u0026gt;] [AS \u0026lt;name\u0026gt;] LABEL\nLABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; MAINTAINER\nMAINTAINER \u0026lt;name\u0026gt; [\u0026lt;email\u0026gt;] RUN\nRUN \u0026lt;command\u0026gt; ADD\nADD [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] \u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt;\rADD [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] [\u0026#34;\u0026lt;src\u0026gt;\u0026#34;,... \u0026#34;\u0026lt;dest\u0026gt;\u0026#34;] COPY\nCOPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] \u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt;\rCOPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] [\u0026#34;\u0026lt;src\u0026gt;\u0026#34;,... \u0026#34;\u0026lt;dest\u0026gt;\u0026#34;] ENV\nENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... CMD Used to provide default command to be run when Docker Container starts from built Image, there can be only 1 CMD directive.\nPractice  Create a folder for container image  mkdir ~/environment/container-image Type cd container-image to change into that directory.  cd ~/environment/container-image Run touch Dockerfile to create the Dockerfile. This file will contain a set of steps needed to build the container image.  touch Dockerfile Run below command to update Dockerfile content  cat \u0026lt;\u0026lt;EOF\u0026gt; Dockerfile\rFROM nginx\\:latest\rCOPY index.html /usr/share/nginx/html\rEOF Run touch index.html to create an empty html file that will contain a simple message.  touch index.html Use echo to pass a simple message into the index.html file  echo \u0026#34;We\u0026#39;ve added our own custom content into the container\u0026#34; \u0026gt;\u0026gt; index.html Use docker build -t nginx:1.0 . to build the nginx container image from the Dockerfile.  docker build -t nginx:1.0 .  Refer to the build image command from a Dockerfile  docker build [OPTIONS] PATH | URL | - Use docker history nginx:1.0 to see all steps and base container.  docker history nginx:1.0  Refer to the command to view the history of an image:  docker history [OPTIONS] IMAGE Use the command docker run -p 8080:80 \u0026ndash;name nginx nginx:1.0 to run the container (don\u0026rsquo;t use background mode for easy debugging)  docker run -p 8080:80 --name nginx nginx:1.0 Open another Terminal tab ( Window -\u0026gt; New Terminal ). Run curl http://localhost:8080 in the tab a few times and see what\u0026rsquo;s new.  curl http://localhost:8080 Go back to the first tab and see the log lines sent immediately to STDOUT. Type Ctrl-C to exit the log output. Note that the container has been stopped but is still there when running docker ps -a.  Using docker ps -a  docker ps -a Use sudo docker inspect nginx to see detailed information about stopped containers.  sudo docker inspect nginx Use docker rm nginx to delete the container  docker rm nginx Mount some files from the host into the container instead of embedding them in the image.  docker run -d -p 8080:80 -v /home/ec2-user/environment/container-image/index.html:/usr/share/nginx/html/index.html\\:ro --name nginx nginx\\: latest Execute curl http://localhost:8080. Note that although this is an upstream nginx image from Docker Hub, the content is there.  curl http://localhost:8080 Edit the index.html. file  echo \u0026#34;This is another line I\u0026#39;ve added to my container\u0026#34; \u0026gt;\u0026gt; index.html Try again curl http://localhost:8080  curl http://localhost:8080 Finally, run docker stop nginx and docker rm nginx to stop and remove the container  docker stop nginx \u0026amp;\u0026amp; docker rm nginx "
},
{
	"uri": "/4-monolith/4.2-mythicalmysfitsmonolith/",
	"title": "Containerize the Mythical Mysfits monolith",
	"tags": [],
	"description": "",
	"content": "Containerize the Mythical Mysfits monolith Meaning of an INSTRUCTION in Dockerfile.  FROM: Used to indicate which image is built from the original image. Depending on the application that needs to be packaged, we will use a different original image. RUN: Used to run a command when building an image. WORKDIR: Used to set the working directory. Any subsequent RUN, CMD, ENTRYPOINT, COPY and ADD instructions will take place inside this WORKDIR directory. COPY: COPY the source directory from the host machine to the filesystem of the image. CMD: Used to provide the default command that will be run when the Docker Container starts from the built Image, there can only be 1 CMD directive.   Check Dockerfile.draft  FROM ubuntu:20.04\rRUN apt-get update -y\rRUN apt-get install -y python3-pip python-dev build-essential\rRUN pip3 install --upgrade pip\r#[TODO]: Copy python source files and requirements file into container image\r#[TODO]: Install dependencies listed in the requirements.txt file using pip3\r#[TODO]: Specify a listening port for the container\r#[TODO]: Run mythicalMysfitsService.py as the final step. We want this container to run as an executable. Looking at ENTRYPOINT and CMD for this? Complete Dockerfile  FROM ubuntu:20.04\rRUN apt-get update -y\rRUN apt-get install -y python3-pip python-dev build-essential\rRUN pip3 install --upgrade pip\r#[TODO]: Copy python source files and requirements file into container image\rCOPY ./service /MythicalMysfitsService\rWORKDIR /MythicalMysfitsService\r#[TODO]: Install dependencies listed in the requirements.txt file using pip3\rRUN pip3 install -r ./requirements.txt\r#[TODO]: Specify a listening port for the container\rEXPOSE 80\r#[TODO]: Run the mythicalMysfitsService.py as the final step\rENTRYPOINT[\u0026#34;python3\u0026#34;]\rCMD [\u0026#34;mythicalMysfitsService.py\u0026#34;] If the Dockerfile is complete, rename your file from \u0026ldquo;Dockerfile.draft\u0026rdquo; to \u0026ldquo;Dockerfile\u0026rdquo; and continue to the next step.  cd ~/environment/amazon-ecs-mythicalmysfits-workshop/workshop-1/app/monolith-service/\rmv Dockerfile.draft Dockerfile Execute build image with command docker build [OPTIONS] PATH | URL | - .  This command needs to be run in the same directory where your Dockerfile is located.\n\r Note the time after that for the build command to look in the current directory for the Dockerfile.  docker build -t monolith-service . You should see a bunch of results as Docker builds all the layers of the image.  If something goes wrong during the build, the build will fail and stop (red text and warnings along the way as long as the build doesn\u0026rsquo;t fail).\n\rRemoving intermediate container a71540b615b4\r---\u0026gt; 5ab93ce927c8\rStep 8/10 : EXPOSE 80\r---\u0026gt; Running in 27074f1d4c3a\rRemoving intermediate container 27074f1d4c3a\r---\u0026gt; f528fe7756d5\rStep 9/10 : ENTRYPOINT [\u0026#34;python3\u0026#34;]\r---\u0026gt; Running in 8ef1757aadb0\rRemoving intermediate container 8ef1757aadb0\r---\u0026gt; a1d1ed159fb2\rStep 10/10 : CMD [\u0026#34;mythicalMysfitsService.py\u0026#34;]\r---\u0026gt; Running in da0c544e601b\rRemoving intermediate container da0c544e601b\r---\u0026gt; b283e0821fc9\rSuccessfully built b283e0821fc9\rSuccessfully tagged monolith-service:latest Your Dockerfile has been successfully created, but the Dockefile has not been optimized for microservices.  Since you convert monoliths into microservices, you will edit the source code (eg mythicalMysfitsService.py) and build this image a few times.\n\r Check Dockerfile  Dockerfile optimization  FROM ubuntu:20.04\rRUN apt-get update -y\rRUN apt-get install -y python3-pip python-dev build-essential\rRUN pip3 install --upgrade pip\rCOPY ./service/requirements.txt .\rRUN pip3 install -r ./requirements.txt\rCOPY ./service /MythicalMysfitsService\rWORKDIR /MythicalMysfitsService\rEXPOSE 80\rENTRYPOINT[\u0026#34;python3\u0026#34;]\rCMD [\u0026#34;mythicalMysfitsService.py\u0026#34;] To see the benefits of Dockerfile optimization, you need to first rebuild the monolith image using the new Dockerfile.  docker build -t monolith-service . Then we make changes to mythicalMysfitsService.py by adding a comment at the end of the file.  # This is a comment to force a Docker-rebuild Docker cached the requests on the first rebuild after reordering.  docker build -t monolith-service . Perform rebuild monolith image again. Cache reference in this second rebuild  Use docker images [OPTIONS] [REPOSITORY[:TAG]] to list images  docker images Run the container and test  TABLE_NAME=$(aws dynamodb list-tables | jq -r .TableNames[0])\rdocker run -p 8000:80 -e AWS_DEFAULT_REGION=$AWS_REGION -e DDB_TABLE_NAME=$TABLE_NAME monolith-service \rNote: You can find your DynamoDB table names in the workshop-1/cfn-output.json file which is derived from the CloudFormation Stack output.\n\rTo test the basic functionality of a monolith service, query the service using a utility like cURL included with Cloud9.   Click the plus sign next to your tabs and select New Terminal or click Window -\u0026gt; New Terminal from the Cloud9 menu to open a new shell session to run the following curl command.  curl http://localhost:8000/mysfits  You will see a JSON array with data about Mythical Mysfits  Note: Processes running inside a Docker container can authenticate with DynamoDB because they can access the EC2 Metadata API endpoint running at 169.254. attached to our Cloud9 environment in the initial setup script. Processes in the container cannot access the ~/.aws/credentials file in the host filesystem (unless it\u0026rsquo;s explicitly attached to the container).\n\rBack to the monolith container running tab   Monolith container running in the foreground with stdout/stderr print to the screen, when receiving the request, will display 200 \u0026ldquo;OK\u0026rdquo;   * Serving Flask app \u0026#34;mythicalMysfitsService\u0026#34; (lazy loading)\r* Environment: production\rWARNING: This is a development server. Do not use it in a production deployment.\rUse a production WSGI server instead.\r* Debug mode: off\r* Running on http://0.0.0.0:80/ (Press CTRL+C to quit)\r172.17.0.1 - - [26/May/2022 16:49:43] \u0026#34;GET /mysfits HTTP/1.1\u0026#34; 200 -\rINFO:werkzeug:172.17.0.1 - - [26/May/2022 16:49:43] \u0026#34;GET /mysfits HTTP/1.1\u0026#34; 200 - In the tab running monolith container, use the key combination Ctrl + C to stop running the container.  Note: Container runs in the foreground with stdout/stderr printing to the console. In a production environment, when running the container in the background and having to configure the destination of the logs. We can run the container in the background using -d.\n\rTABLE_NAME=$(aws dynamodb list-tables | jq -r .TableNames[0])\rdocker run -d -p 8000:80 -e AWS_DEFAULT_REGION=$AWS_REGION -e DDB_TABLE_NAME=$TABLE_NAME monolith-service List docker containers to check running containers  docker ps See the running monolith in the list (store the Container ID to use docker logs). Repeat the curl command, then do a log check  docker logs \u0026lt;CONTAINER_ID\u0026gt; Now, we have Docker image working, we do tag assignment and push to ECR. AWS ECR is a fully managed AWS Docker container registry service that simplifies the storage, management, and deployment of Docker container images. ECR can be integrated with Amazon Elastic Container Service (ECS) to simplify deployment execution flow setup for production systems as well as eliminate management complexity. Repository manager for container images. Next, we use the ECS pull image from the ECR.   Go to ECS and select Repositories We will have 2 repositories: STACK_NAME-mono-xxx and STACK_NAME-like-xxx Select the icon to copy URL of Mono repository (use in the next steps)  Note: repository URI is unique\n\rImplement tag assignment and push container image and monolith repository  MONO_ECR_REPOSITORY_URI=$(aws ecr describe-repositories | jq -r .repositories[].repositoryUri | grep mono)\rdocker tag monolith-service:latest $MONO_ECR_REPOSITORY_URI:latest\rdocker push $MONO_ECR_REPOSITORY_URI:latest Visit the ECR repository page, the image is uploaded and tagged as the latest.  "
},
{
	"uri": "/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "Preparation In the lab, we will prepare the resources using CloudFormation for deployment.\nAn AWS IAM account with enhanced privileges allows you to interact with CloudFormation, IAM, EC2, ECS, ECR, ELB/ALB, VPC, SNS, CloudWatch, Cloud9. You refer to more about IAM\n\r Create CloudFormation Stack Make Setup Generate SSH Key  "
},
{
	"uri": "/7-microservices/7.2-updateservice/",
	"title": "Update Service",
	"tags": [],
	"description": "",
	"content": "Update Service  Continue, access ECS. We will proceed to update the service.   Select Cluster Select Cluster-STACK_NAME  In the cluster,   Select Service Select the existing Service (STACK_NAME-MythicalMonolithService-xxx) Select Edit  Make changes to the latest revision (the one we just created).   Select Update  Thus, we have successfully created a new revision and updated the service.  Access ECS   Select Cluster Select Service Select Monolith-Definition-STACK_NAME revision 3.  In the Cluster-STACK_NAME interface   Select Task Select Monolith-Definition-STACK_NAME revision 3.  Check Logs  "
},
{
	"uri": "/7-microservices/7.3-builddockerimage/",
	"title": "Build Docker Image",
	"tags": [],
	"description": "",
	"content": " Access to ECR   Select Repository Export 2 repositories STACK_NAME-like-XXX and STACK_NAME-mono-XXX  Now will build like service   cd ~/environment/amazon-ecs-mythicalmysfits-workshop/workshop-1/app/like-service\rLIKE_ECR_REPOSITORY_URI=$(aws ecr describe-repositories | jq -r .repositories[].repositoryUri | grep like)\rdocker build -t like-service . Successfully execute docker image build then push to ECR with the latest tag.   docker tag like-service:latest $LIKE_ECR_REPOSITORY_URI:latest\rdocker push $LIKE_ECR_REPOSITORY_URI:latest Executing the push successfully.  Check pushed image in STACK_NAME-like-XXX  "
},
{
	"uri": "/2-prerequiste/2.3-createanssh/",
	"title": "Generate SSH Key",
	"tags": [],
	"description": "",
	"content": "Generate SSH key  We execute the command to generate SSH key in Cloud9. This key will be used to access the node instance  We can access it with the command: ssh -i PRIVATE_KEY.PEM ec2-user@EC2_PUBLIC_DNS_NAME\n\rssh-keygen We do upload public key to EC2 region  aws ec2 import-key-pair --key-name \u0026#34;mythicaleks\u0026#34; --public-key-material file://~/.ssh/id_rsa.pub \rIf you get the error An error occurred (InvalidKey.Format) when calling the ImportKeyPair operation: Key is not in valid OpenSSH public key format, you can try the following command:\n\raws ec2 import-key-pair --key-name \u0026#34;mythicaleks\u0026#34; --public-key-material fileb://~/.ssh/id_rsa.pub Go to EC2   Select Key pair View keys that have been uploaded to EC2 region  "
},
{
	"uri": "/3-containersanddocker/",
	"title": "Introducing Docker and Containers",
	"tags": [],
	"description": "",
	"content": "Introducing Docker and Containers In this section, we will practice some basic commands about Docker and understand the concept of Container.\nRun Docker on AWS AWS provides support for both Docker commercial and open source solutions. There are many ways to run containers on AWS, including Amazon Elastic Container Service (ECS) which is an extremely flexible and high-performance container management service. Customers can easily deploy their containerized applications from their local Docker environment straight to Amazon ECS.\nAWS Fargate is a technology for Amazon ECS that allows you to run containers in production without needing to deploy or manage infrastructure. AWS Fargate is technology for Amazon ECS that allows you to run containers without provisioning or managing servers.\nAmazon Elastic Container Registry (ECR) is a secure and highly available private container repository that makes it easy to store and manage your Docker container images, encrypting and compressing images in storage for future use. Extraction is fast and secure.\nContent  Docker Basic Container image  "
},
{
	"uri": "/4-monolith/",
	"title": "Containerize monolith",
	"tags": [],
	"description": "",
	"content": "Containerize the Mythical Mysfits monolith  CloudFormation Check Containerize the Mythical Mysfits monolith  "
},
{
	"uri": "/7-microservices/7.4-createtaskdefinition/",
	"title": "Create Task Definition",
	"tags": [],
	"description": "",
	"content": "Create Task Definition  Create Task Definition for like service using image pushed to ECR.   In the ECS interface, select Task definitions Select Create new task definition  Configure Task definition   Set Task definition family, enter Microservice-Definition-STACK_NAME  We use the image pushed to ECR in the repository STACK_NAME-like-XXX   Like service code designed to call an endpoint on a monolith to persist data with DynamoDB. Reference the value of the MONOLITH_URL environment. We will create the environment with the Key of MONOLITH_URL and Value of ALB (specifically alb-STACK_NAME-XXX) Select Next  Perform environment configuration   We use AWS Fargate Operating system is Linux You can customize the Task size Select Task role, select STACK_NAME-EcsServiceRole-XXX  Use Amazon CloudWatch  Check again and select Create  Thus, we have successfully created a Task definition Microservice-Definition-STACK_NAME  "
},
{
	"uri": "/7-microservices/7.5-createecsservice/",
	"title": "Create ECS Service",
	"tags": [],
	"description": "",
	"content": " Also in the newly created Task interface, we will create an ECS service to run the newly created Task definition.   Select Deploy Select Create Service  Setting up the environment   Select cluster (Cluster-STACK_NAME)  Perform configuration *Deployment   Application type, default Service Perform naming Service name Desired tasks, choose 1  Configure Load Balancing   Select Application Load Balancer Select Use an existing load balancer Select alb-STACK_NAME Configure Listener (Port: 80 and Protocol: HTTP) Create Target group, enter microservice-tg Health check path, enter / Health check grace period, enter 300  Configure Network   VPC, select Mysfits-STACK_NAME Subnets, choose private subnet Go to Security group, select Use an existing security group We choose Security group default and make sure to configure inbound HTTP (port 80) Select Deploy  Thus, we create the service deploy successfully.  After the Microservice service deploys, we test the website interface and perform the like test.  We use the browser to access the website. Check CloudWatch logs again and show \u0026ldquo;Like processed.\u0026rdquo;  Implement endpoint removal from monolith using Cloud9.   In the monolith folder, open mythicalMysfitsService.py find the following code:  # increment the number of likes for the provided mysfit.\r@app.route(\u0026#34;/mysfits/\u0026lt;mysfit_id\u0026gt;/like\u0026#34;, methods=[\u0026#39;POST\u0026#39;])\rdef likeMysfit(mysfit_id):\rserviceResponse = mysfitsTableClient.likeMysfit(mysfit_id)\rprocess_like_request()\rflaskResponse = Response(serviceResponse)\rflaskResponse.headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json\u0026#34;\rreturn flaskResponse Then we build a monolith image   cd ~/environment/amazon-ecs-mythicalmysfits-workshop/workshop-1/app/monolith-service\rdocker build -t monolith-service:nolike2 . Perform tag assignment and push to monolith ECR repository.   We use the tag: nolike2.   docker tag monolith-service:nolike2 $MONO_ECR_REPOSITORY_URI:nolike2\rdocker push $MONO_ECR_REPOSITORY_URI:nolike2 Check monolith repository on ECR, we will see that the image is pushed with the tag nolike2.  Now make one last Task Definition for the monolith to refer to this new container image URI (this process should be familiar now, and you can probably see that it makes sense to leave this drudgery to a CI/CD service in production), update the monolith service to use the new Task Definition, and make sure the app still functions as before  "
},
{
	"uri": "/5-amazonecsandawsfargate/",
	"title": "Deploy the container using AWS Fargate",
	"tags": [],
	"description": "",
	"content": "Deploy the container using AWS Fargate  First we will create Task definitions to run monolith   Go to ECS Select Task definitions Find the Task definition named Monolith-Definition-STACK_NAME  Go to ECR   Copy Image URI  Back to interface ECS   Select Monolith-Definition-STACK_NAME  In the interface Monolith-Definition-STACK_NAME revision 1. We will create a new revision.   Select Create new revision  Configure Container   Name, enter the service name of your choice (In the lab, enter monolith-service) Paste the copied Image URI into Image URI  Select Create  Finish creating a new revision  In view Monolith-Definition-STACK_NAME revision 2   Select Deploy Select Run task  In the Deploy interface   Environment, select Existing cluster, select Cluster-STACK_NAME Select Launch type In Launch type, select FARGATE Platform version select LATEST  In the Deployment configuration section   Select Task Desired choose 1  Configure Networking   VPC, select Mysfits-VPC-STACK_NAME Subnets select Mysfits-PublicOne-STACK_NAME Select Security Group, Select Use an existing security group Security group name, choose default but configure inbound port 80 Auto-assign public IP - \u0026ldquo;ENABLED\u0026rdquo; Select Deploy  Create task successfully  Select the task you just created and select Networking.   Purpose of using Public IP to use curl command to check by making GET request. When using Fargate initialization, each task gets its own ENI and Public IP and Private IP.  Make a browser access  http://TASK_PUBLIC_IP_ADDRESS/mysfits Execute the curl command on Cloud9  curl http://TASK_PUBLIC_IP_ADDRESS/mysfits Select the task you just created and select Logs  Using AWS CloudWatch to View Log events   Select monolith log group (STACK_NAME-MythicalMonolithLogGroup-XXX)  After running the curl command successfully, we can delete the task   Select Task Select Monolith-Definition-STACK_NAME revision 2. Select Stop Select Stop selected  Verify Stop and select Stop  Delete task successfully.  "
},
{
	"uri": "/6-alb/",
	"title": "ALB and ECS Service",
	"tags": [],
	"description": "",
	"content": "Scale the adoption platform monolith with an ALB and an ECS Service The Run Task method used in the previous section is great for testing, but if needed to run the platform apply it as a long-running process. You need to use Elastic Load Balancing Application Load Balancer (ALB) to deliver requests to your running containers. In addition to simple load balancing, it also creates capabilities such as path-based routing to different services.\nECS helps maintain the number of desired tasks (the number of containers running for a long time) and integrates ALB (handles the registration/deregistering of containers into the ALB)\nThe original ECS and ALB service was created by CloudFormation. In this lab, you will update those resources to host the encapsulated monolith service. Then you will create a new service from scratch after breaking the monolith.\n Access to CloudFormation   Select STACK_NAME Select Stack details Select Outputs Select LoadBalancerDNS  Use your browser to access LoadBalancerDNS  Go to ECS   Select Clusters Select Cluster-STACK_NAME  In the Cluster-STACK_NAME interface, we proceed to update service   Select Services Select Edit  In the service update interface   Select Revision 2 Select Update  Service update is successful.  In the Cluster-STACK_NAME interface   Select Task Check Monolith-Definition-STACK_NAME has been updated Revision 2  Access to CloudFormation   Select STACK_NAME Select Stack details Select Outputs Select S3WebsiteURL  Use a browser to access S3WebsiteURL  Perform user interface experience operations  Access to ECS   Select Cluster Select Cluster-STACK_NAME Select Tasks Select Monolith-Definition-STACK_NAME revision 2  Select Logs  Check the logs to make sure the monolith can read and write DynamoDB and can handle the like.  Check CloudWatch logs from ECS to make sure Like processed  Access CloudWatch   Select Log groups Observe event log  To distinguish between service and task. We can do the following steps:   Select Service Select Edit  Increase Desired task to 3 and update.  After a successful update.   Select Task to see 3 running tasks. From there, we see ECS services is a concept in which ECS allows running and maintaining a specific number of containers of task definitions in an ECS cluster. A service consists of many tasks and is maintained.  "
},
{
	"uri": "/7-microservices/",
	"title": "Microservices with AWS Fargate",
	"tags": [],
	"description": "",
	"content": "Microservices with AWS Fargate In this lab, we will break the monolith into microservices.\nMonolith provides different API resources on other routes to fetch information about Mysfits, like or adopt experiences\nThe logic of these resources includes some proccess (like making sure that the user is authorized to perform a specific action, that Mysfit is eligible to apply, etc.) with persistence layer (specifically DynamoDB).\nInstead of using many different services that interact directly with a database (adding indexes and data migrations is difficult with an application). So do not separate all the logic of the resource into a separate service, but only move the Process logic to a separate service and still use monolith as a premise with the database (ie, do not completely switch from monolith to microservice but only transfer but the complex part).\nALB has another feature called path-based routing , which routes traffic based on URL paths to target groups. This means you will only have single instance of ALB to host your microservices. The monolith service will receive all traffic to the default path \u0026rsquo;/\u0026rsquo;. Adoption and like service will be \u0026rsquo;/accept\u0026rsquo; and \u0026lsquo;/like\u0026rsquo; respectively.\nContent  Create Revision Update Service Build Docker Image Create Task Definition Create ECS Service  "
},
{
	"uri": "/8-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Clean up resources Remove ECS service   Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n  On the navigation bar, select the Region where your cluster is located.\n  In the navigation pane, select Cluster and select the name of the Cluster associated with the lab.\n  On the Cluster page, select Service\n  Select Delete.\n  Confirm delete service.\n  Delete repository   Access ECR: https://console.aws.amazon.com/ecr/repositories.\n  Select Region containing the repository\n  Select Repository\n  Select Private and select the repositories related to the lab.\n  Select Delete\n  Delete ALB  Access to EC2 Select Load Balancer Select ALB of the lab. Select Actions Select Delete  Clear CloudFormation Stack  Access to CloudFormation Select Stack Select the stack related to the lab. Select Delete  "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]